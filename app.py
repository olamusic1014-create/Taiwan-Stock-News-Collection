import streamlit as st
import asyncio
from playwright.async_api import async_playwright
import time
import random
import sys
import xml.etree.ElementTree as ET
import os
import subprocess

# === é›²ç«¯ç’°å¢ƒå°ˆç”¨ï¼šè‡ªå‹•å®‰è£ Chromium ç€è¦½å™¨ ===
# é€™è¡Œä»£ç¢¼æœƒæª¢æŸ¥æ˜¯å¦åœ¨é›²ç«¯ï¼Œå¦‚æœæ˜¯ï¼Œå°±è‡ªå‹•å®‰è£ç€è¦½å™¨
try:
    subprocess.run(["playwright", "install", "chromium"], check=True)
except Exception as e:
    pass
# ===========================================

# === Windows ç³»çµ±å°ˆç”¨ä¿®å¾© ===
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# ===========================
# 1. çˆ¬èŸ²æ ¸å¿ƒ (V11.5 é›²ç«¯éƒ¨å±¬ç‰ˆ)
# ===========================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
]
def get_ua(): return random.choice(USER_AGENTS)

async def fetch_stock_name(stock_code):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            # ç­–ç•¥ A: HiStock
            await page.goto(f"https://histock.tw/stock/{stock_code}", timeout=10000)
            title = await page.title()
            if "(" in title and ")" in title:
                return title.split("(")[0].strip()

            # ç­–ç•¥ B: Goodinfo
            await page.goto(f"https://goodinfo.tw/tw/StockDetail.jsp?STOCK_ID={stock_code}", timeout=10000)
            g_title = await page.title()
            if "(" in g_title:
                return g_title.split("(")[0].strip()
            return stock_code 
        except: return stock_code
        finally: await browser.close()

async def fetch_google_rss(stock_code, site_domain, source_name):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            rss_url = f"https://news.google.com/rss/search?q={stock_code}+site:{site_domain}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
            response = await page.goto(rss_url, timeout=20000, wait_until="commit")
            xml_content = await response.text()
            root = ET.fromstring(xml_content)
            data = []
            for item in root.findall('.//item'):
                title = item.find('title').text
                clean_title = title.split(" - ")[0]
                if len(clean_title) > 6:
                    data.append({"title": clean_title, "source": source_name})
            return data[:5]
        except: return []
        finally: await browser.close()

async def scrape_anue(stock_code):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            await page.goto(f"https://www.cnyes.com/search/news?q={stock_code}", timeout=15000, wait_until="commit")
            await page.wait_for_timeout(1500)
            titles = await page.locator('h3, h2').all_inner_texts()
            data = [{"title": t, "source": "é‰…äº¨ç¶²"} for t in titles if len(t) > 6 and "è‚¡åƒ¹" not in t][:5]
            return data
        except: return []
        finally: await browser.close()

async def scrape_yahoo(stock_code):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(user_agent=get_ua())
        page = await context.new_page()
        try:
            await page.goto(f"https://tw.stock.yahoo.com/quote/{stock_code}.TW/news", timeout=20000, wait_until="domcontentloaded")
            await page.wait_for_timeout(1500)
            titles = await page.locator('#YDC-Stream li h3').all_inner_texts()
            if not titles: titles = await page.locator('#YDC-Stream li a').all_inner_texts()
            data = [{"title": t, "source": "Yahoo"} for t in titles if len(t) > 5 and "å»£å‘Š" not in t][:5]
            return data
        except: return []
        finally: await browser.close()

async def scrape_udn(stock_code): return await fetch_google_rss(stock_code, "money.udn.com", "ç¶“æ¿Ÿæ—¥å ±")
async def scrape_ltn(stock_code): return await fetch_google_rss(stock_code, "ec.ltn.com.tw", "è‡ªç”±è²¡ç¶“")
async def scrape_ctee(stock_code): return await fetch_google_rss(stock_code, "ctee.com.tw", "å·¥å•†æ™‚å ±")
async def scrape_chinatimes(stock_code): return await fetch_google_rss(stock_code, "chinatimes.com", "ä¸­æ™‚æ–°è")
async def scrape_ettoday(stock_code): return await fetch_google_rss(stock_code, "ettoday.net", "ETtoday")
async def scrape_tvbs(stock_code): return await fetch_google_rss(stock_code, "news.tvbs.com.tw", "TVBSæ–°è")
async def scrape_businesstoday(stock_code): return await fetch_google_rss(stock_code, "businesstoday.com.tw", "ä»Šå‘¨åˆŠ")
async def scrape_wealth(stock_code): return await fetch_google_rss(stock_code, "wealth.com.tw", "è²¡è¨Š")
async def scrape_storm(stock_code): return await fetch_google_rss(stock_code, "storm.mg", "é¢¨å‚³åª’")

def calculate_score(news_list, source_name):
    if not news_list: return 0, []
    positive = ["ä¸Šæ¼²", "é£†", "å‰µé«˜", "è²·è¶…", "å¼·å‹¢", "è¶…é æœŸ", "å–å¾—", "è¶…è¶Š", "åˆ©å¤š", "æˆé•·", "æ”¶ç›Š", "å™´", "æ¼²åœ", "æ—º", "æ”»é ‚", "å—æƒ ", "çœ‹å¥½", "ç¿»ç´…", "é©šè‰·", "AI", "æ“´ç”¢", "å…ˆé€²", "å‹•èƒ½", "ç™¼å¨", "é ˜å…ˆ", "æ¶å–®", "å­£å¢", "å¹´å¢", "æ¨‚è§€", "å›æº«", "å¸ƒå±€", "åˆ©æ½¤", "å¤§æ¼²"]
    negative = ["ä¸‹è·Œ", "è³£", "ç ", "è§€æœ›", "ä¿å®ˆ", "ä¸å¦‚", "é‡æŒ«", "å¤–è³‡è³£", "ç¸®æ¸›", "å´©", "è·Œåœ", "ç–²è»Ÿ", "åˆ©ç©º", "ä¿®æ­£", "èª¿ç¯€", "å»¶å¾Œ", "è¡°é€€", "ç¿»é»‘", "ç¤ºè­¦", "é‡æ®º", "ä¸å¦‚é æœŸ", "è£å“¡", "è™§æ", "å¤§è·Œ", "é‡æŒ«"]
    score = 50
    reasons = []
    for news in news_list:
        title = news['title']
        hit = False
        for w in positive:
            if w in title: score += 12; reasons.append(w); hit = True
        for w in negative:
            if w in title: score -= 12; reasons.append(w); hit = True
        if not hit and len(title) > 5: score += 2
    return max(0, min(100, score)), list(set(reasons))

async def run_analysis(stock_code):
    return await asyncio.gather(scrape_anue(stock_code), scrape_yahoo(stock_code), scrape_udn(stock_code), scrape_ltn(stock_code), scrape_ctee(stock_code), scrape_chinatimes(stock_code), scrape_ettoday(stock_code), scrape_tvbs(stock_code), scrape_businesstoday(stock_code), scrape_wealth(stock_code), scrape_storm(stock_code))

st.set_page_config(page_title="V11.5 é›²ç«¯è‚¡ç¥¨ç†±åº¦å„€", page_icon="ğŸ“ˆ", layout="wide")
st.markdown("""<style>.source-tag { padding: 3px 6px; border-radius: 4px; font-size: 11px; margin-right: 5px; color: white; display: inline-block; }.news-row { margin-bottom: 8px; padding: 4px; border-bottom: 1px solid #333; font-size: 14px; }.stock-check { background-color: #262730; padding: 10px; border-radius: 5px; border: 1px solid #4b4b4b; text-align: center; margin-bottom: 15px; }.stock-name-text { font-size: 24px; font-weight: bold; color: #4CAF50; }</style>""", unsafe_allow_html=True)

st.title("ğŸ“ˆ V11.5 è‚¡å¸‚å…¨è¦–è§’ç†±åº¦ç›£æ¸¬ (é›²ç«¯ç‰ˆ)")
st.markdown("æ•´åˆ **11 å¤§æ¬Šå¨åª’é«”**ï¼Œæ”¯æ´æ‰‹æ©Ÿ/é›»è…¦è·¨å¹³å°ä½¿ç”¨ã€‚")

with st.sidebar:
    st.header("âš™ï¸ è‚¡ç¥¨è¨­å®š")
    stock_input = st.text_input("è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼ (æŒ‰ Enter ç¢ºèª)", value="2330")
    if stock_input:
        if 'last_stock' not in st.session_state or st.session_state.last_stock != stock_input:
            with st.spinner(f"æ­£åœ¨ç¢ºèª {stock_input} ..."):
                stock_name = asyncio.run(fetch_stock_name(stock_input))
                st.session_state.stock_name_display = stock_name
                st.session_state.last_stock = stock_input
        if st.session_state.get('stock_name_display'):
            st.markdown(f"<div class='stock-check'><div style='font-size: 12px; color: #aaa;'>ç¢ºèªç›®æ¨™</div><div class='stock-name-text'>{st.session_state.stock_name_display}</div><div style='font-size: 12px; color: #888;'>({stock_input})</div></div>", unsafe_allow_html=True)
        else: st.markdown(f"<div class='stock-check' style='color:#ff4757'>âš ï¸ æŸ¥ç„¡æ­¤ä»£è™Ÿ</div>", unsafe_allow_html=True)
    run_btn = st.button("ğŸš€ å•Ÿå‹•é›²ç«¯æƒæ", type="primary")

if run_btn and stock_input:
    status = st.empty(); bar = st.progress(0)
    status.text(f"ğŸ” é›²ç«¯ä¸»æ©Ÿæ­£åœ¨é€£ç·š 11 å¤§æ•¸æ“šæº...")
    bar.progress(10)
    results = asyncio.run(run_analysis(stock_input))
    bar.progress(85)
    status.text("ğŸ§  æ­£åœ¨è¨ˆç®—æƒ…ç·’...")
    source_names = ["é‰…äº¨ç¶²", "Yahoo", "ç¶“æ¿Ÿæ—¥å ±", "è‡ªç”±è²¡ç¶“", "å·¥å•†æ™‚å ±", "ä¸­æ™‚æ–°è", "ETtoday", "TVBSæ–°è", "ä»Šå‘¨åˆŠ", "è²¡è¨Š", "é¢¨å‚³åª’"]
    data_map = {name: res for name, res in zip(source_names, results)}
    scores = {}; all_signals = []; all_news = []; valid_count = 0; total_score = 0
    for name, data in data_map.items():
        s, r = calculate_score(data, name)
        scores[name] = s; all_signals.extend(r); all_news.extend(data)
        if len(data) > 0: total_score += s; valid_count += 1
    final_score = round(total_score / valid_count, 1) if valid_count > 0 else 0
    bar.progress(100); time.sleep(0.5); status.empty(); bar.empty()

    col1, col2, col3 = st.columns([1, 1, 1])
    with col1: st.metric("å…¨å¸‚å ´ç†±åº¦", f"{final_score} åˆ†", f"{len(all_news)} å‰‡æ–°è")
    with col2:
        if final_score >= 75: l, c = "ğŸ”¥ğŸ”¥ğŸ”¥ æ²¸é¨°", "#ff4757"
        elif final_score >= 60: l, c = "ğŸ”¥ åŠ æº«", "#ffa502"
        elif final_score <= 35: l, c = "ğŸ§Š å†°å‡", "#5352ed"
        else: l, c = "âš–ï¸ æº«å’Œ", "#747d8c"
        st.markdown(f"<h2 style='color:{c}'>{l}</h2>", unsafe_allow_html=True)
    with col3: st.write(", ".join(list(set(all_signals))[:15]) if all_signals else "ç„¡è¨Šè™Ÿ")
    st.divider()
    c1, c2 = st.columns(2)
    with c1:
        keys = list(data_map.keys())
        for name in keys[:6]: 
            s = scores[name]; cnt = len(data_map[name])
            if cnt: st.write(f"**{name}**: {s}"); st.progress(s)
            else: st.caption(f"{name}: âš ï¸")
    with c2:
        for name in keys[6:]:
            s = scores[name]; cnt = len(data_map[name])
            if cnt: st.write(f"**{name}**: {s}"); st.progress(s)
            else: st.caption(f"{name}: âš ï¸")
    st.divider()
    if all_news:
        cmap = {"é‰…äº¨ç¶²": "#0984e3", "Yahoo": "#6c5ce7", "ç¶“æ¿Ÿæ—¥å ±": "#e17055", "è‡ªç”±è²¡ç¶“": "#d63031", "å·¥å•†æ™‚å ±": "#00b894", "ä¸­æ™‚æ–°è": "#e84393", "ETtoday": "#fdcb6e", "TVBSæ–°è": "#2d3436", "ä»Šå‘¨åˆŠ": "#00cec9", "è²¡è¨Š": "#fab1a0", "é¢¨å‚³åª’": "#636e72"}
        for n in all_news[:30]:
            bg = cmap.get(n['source'], "#999")
            st.markdown(f"<div class='news-row'><span class='source-tag' style='background-color:{bg}'>{n['source']}</span><a href='https://www.google.com/search?q={n['title']}' target='_blank' style='text-decoration:none; color:inherit'>{n['title']}</a></div>", unsafe_allow_html=True)
    else: st.info("ç„¡æ–°è")